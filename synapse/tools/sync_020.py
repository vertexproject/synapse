'''
Sync splices from an 0.1.x cortex to 0.2.x
'''
import os
import sys
import asyncio
import logging
import argparse

import synapse.common as s_common
import synapse.telepath as s_telepath

import synapse.lib.cell as s_cell
import synapse.lib.base as s_base
import synapse.lib.queue as s_queue
import synapse.lib.layer as s_layer
import synapse.lib.config as s_config
import synapse.lib.output as s_output

logger = logging.getLogger(__name__)

class SyncMigratorApi(s_cell.CellApi):
    '''
    A telepath/cell API for the Sync service.
    '''
    async def status(self):
        return await self.cell.status()

    async def startSyncFromFile(self):
        return await self.cell.startSyncFromFile()

    async def startSyncFromLast(self):
        return await self.cell.startSyncFromFile()

class SyncMigrator(s_cell.Cell):
    cellapi = SyncMigratorApi
    confdefs = {
        'src': {
            'type': 'string',
            'description': 'Telepath URL for the source 0.1.x cortex.',
        },
        'dest': {
            'type': 'string',
            'description': 'Telepath URL for the destination 0.2.x cortex.',
        },
        'offsfile': {
            'type': 'string',
            'description': 'File path for the YAML file containing layer offsets.',
        },
        'poll_s': {
            'type': 'integer',
            'description': 'The number of seconds to wait between calls to src for new splices.',
            'default': 60,
        },
        'batch_size': {
            'type': 'integer',
            'description': 'The number of nodeedits to batch before pushing to dest cortex.',
            'default': 10,
        }
    }

    async def __anit__(self, dirn, conf=None):
        await s_cell.Cell.__anit__(self, dirn, conf=conf)

        self.src = self.conf.get('src')
        self.dest = self.conf.get('dest')
        self.offsfile = self.conf.get('offsfile')  # TODO
        self.poll_s = self.conf.get('poll_s')
        self.batch_size = self.conf.get('batch_size')

        self.pull_fair_iter = 100
        self.push_fair_iter = 100
        self.err_lim = 10

        self.pull_offs = await self.hive.dict(('sync:pulloffs', ))
        self.push_offs = await self.hive.dict(('sync:pushoffs', ))
        self.errors = await self.hive.dict(('sync:errors', ))  # TODO

        self.model = {}

        self._pull_tasks = {}  # lyriden: task
        self._push_tasks = {}  # lyriden: task

        self.pull_last_start = {}
        self.push_last_start = {}

        self._queues = {}  # lyriden: queue of splices

    async def status(self):
        pass  # TODO

    async def startSyncFromFile(self):
        '''
        Start sync from layer offsets provided in offsfile generated by migration tool, e.g.
            <lyriden>
                created: <epochms>
                nextoffs: <int>
        '''
        lyroffs = s_common.yamlload(self.offsfile)

        for lyriden, info in lyroffs.items():
            nextoffs = info['nextoffs']
            logger.info(f'Starting Layer sync for {lyriden} from file offset {nextoffs}')
            await self._startLyrSync(lyriden, nextoffs)

    async def startSyncFromLast(self):
        '''
        Start sync from minimum last offset stored by push and pull.
        This can also be used to restart dead tasks.
        '''
        for lyriden, pulloffs in self.pull_offs.items():
            pushoffs = await self.getLyrOffset('push', lyriden)
            if pushoffs is None:
                nextoffs = pulloffs
            else:
                nextoffs = min(pulloffs, pushoffs)

            logger.info(f'Starting Layer sync for {lyriden} from last offset {nextoffs}')
            await self._startLyrSync(lyriden, nextoffs)

    async def _startLyrSync(self, lyriden, nextoffs):
        '''
        Starts up the sync process for a given layer and starting offset.
        Always retrieves a fresh datamodel.
        Creates layer queue and fires layer push/pull tasks if they do not already exist.

        Args:
            lyriden (str): Layer iden
            nextoffs (int): The layer offset to start sync from
        '''
        await self._setLyrOffset('pull', lyriden, nextoffs)

        await self._loadDatamodel()

        queue = self._queues.get(lyriden)
        if queue is None:
            queue = await s_queue.Window.anit(maxsize=None)
            self.onfini(queue.fini)
            self._queues[lyriden] = queue

        pulltask = self._pull_tasks.get(lyriden)
        if pulltask is None or pulltask.done():
            self._pull_tasks[lyriden] = self.schedCoro(self._srcPullLyrSplices(lyriden))

        pushtask = self._push_tasks.get(lyriden)
        if pushtask is None or pushtask.done():
            self._push_tasks[lyriden] = self.schedCoro(self._destPushLyrNodeedits(lyriden))

    async def _setLyrOffset(self, pushorpull, lyriden, offset):
        '''
        Stores the next offset to be read for a given layer.

        Args:
            pushorpull (str): "pull" or "push" to define context for stored offset
            lyriden (str): Layer iden
            offset (int): The offset to start sync from
        '''
        if pushorpull == 'pull':
            await self.pull_offs.set(lyriden, offset)
        elif pushorpull == 'push':
            await self.push_offs.set(lyriden, offset)

    async def getLyrOffset(self, pushorpull, lyriden):
        '''
        Retrieve the next layer offset to be read.

        Args:
            pushorpull (str): "pull" or "push" to define context for stored offset
            lyriden (str): Layer iden

        Returns:
            (int or None): Next offset or None if layer offset does not exist
        '''
        if pushorpull == 'pull':
            return self.pull_offs.get(lyriden, default=None)
        elif pushorpull == 'push':
            return self.push_offs.get(lyriden, default=None)

    async def _setLyrErr(self, lyriden, offset, err):
        errs = await self.getLyrErrs(lyriden)
        errs[offset] = err
        await self.errors.set(lyriden, errs)

    async def getLyrErrs(self, lyriden):
        return self.errors.get(lyriden, default={})

    async def _loadDatamodel(self):
        '''
        Retrieve the datamodel (with stortypes) from the destination cortex.
        '''
        async with await s_telepath.openurl(self.dest) as prx:
            model = await prx.getModelDict()
            self.model.update(model)

    async def _srcPullLyrSplices(self, lyriden):
        '''
        Open a proxy to the source layer and initiates splice reader.
        Intended to be run as a fired task, and will poll for updates every poll_s.

        Args:
            lyriden (str): Layer iden
        '''
        poll_s = self.poll_s
        async with await s_telepath.openurl(os.path.join(self.src, 'layer', lyriden)) as prx:
            while not self.isfini:
                queue = self._queues.get(lyriden)
                startoffs = await self.getLyrOffset('pull', lyriden)

                logger.info(f'Pulling splices for layer {lyriden} starting from offset {startoffs}')
                self.pull_last_start[lyriden] = s_common.now()
                nextoffs = await self._srcIterLyrSplices(prx, startoffs, queue)

                await self._setLyrOffset('pull', lyriden, nextoffs)

                logger.info(f'All splices from {lyriden} have been read; offsets: {startoffs} -> {nextoffs}')
                await asyncio.sleep(poll_s)

    async def _srcIterLyrSplices(self, prx, startoffs, queue):
        '''
        Iterate over available splices for a given source layer proxy, and push into queue

        Args:
            prx (s_telepath.Proxy): Proxy to source layer
            startoffs (int): Offset to start iterating from
            queue (s_queue.Window): Layer queue for splices

        Returns:
            (int): Next offset to start from when all splices have been read
        '''
        curoffs = startoffs
        fair_iter = self.pull_fair_iter
        async for splice in prx.splices(startoffs, -1):
            await queue.put((curoffs, splice))

            curoffs += 1

            if curoffs % fair_iter == 0:
                await asyncio.sleep(0)

        return curoffs

    async def _trnNodeSplicesToNodeedit(self, ndef, splices):
        '''
        Translate a batch of splices for a given node into a nodeedit set

        Args:
            ndef (tuple): (<form>, <valu>)
            splices (list): [ (<edit>, {<splice info>}), ...]

        Returns:
            (tuple): (cond, nodeedit, meta)
                cond: None or error dict
                nodeedit: (<buid>, <form>, [edits]) where edits is list of (<type>, <info>)
                meta: nodeedit meta dict
        '''
        buid = s_common.buid(ndef)
        form = ndef[0]
        fval = ndef[1]
        meta = None

        stype_f = await self._destGetStortype(form=form)
        if stype_f is None:
            err = {'mesg': f'Unable to determine stortype type for form {form}', 'splices': splices}
            logger.warning(err['mesg'])
            return err, None, None

        edits = []

        for splice in splices:
            spedit = splice[0]
            props = splice[1]

            # by definition all of these splices have the same meta (same node and same prov)
            if meta is None:
                meta = {k: v for k, v in props.items() if k in ('time', 'user', 'prov')}

            if spedit == 'node:add':
                edit = s_layer.EDIT_NODE_ADD
                edits.append((edit, (fval, stype_f)))

            elif spedit == 'node:del':
                edit = s_layer.EDIT_NODE_DEL
                edits.append((edit, (fval, stype_f)))

            elif spedit in ('prop:set', 'prop:del'):
                prop = props.get('prop')
                pval = props.get('valu')

                stype_p = await self._destGetStortype(form=form, prop=prop)
                if stype_p is None:
                    err = {'mesg': f'Unable to determine stortype type for prop {form}:{prop}', 'splice': splice}
                    logger.warning(err)
                    return err, None, None

                if spedit == 'prop:set':
                    edit = s_layer.EDIT_PROP_SET
                    edits.append((edit, (prop, pval, None, stype_p)))

                elif spedit == 'prop:del':
                    edit = s_layer.EDIT_PROP_DEL
                    edits.append((edit, (prop, pval, stype_p)))

            elif spedit in ('tag:add', 'tag:del'):
                tag = props.get('tag')
                tval = props.get('valu')
                toldv = props.get('oldv')

                if spedit == 'tag:add':
                    edit = s_layer.EDIT_TAG_SET
                    edits.append((edit, (tag, tval, toldv)))

                elif spedit == 'tag:del':
                    edit = s_layer.EDIT_TAG_DEL
                    edits.append((edit, (tag, tval)))

            elif spedit in ('tag:prop:set', 'tag:prop:del'):
                tag = props.get('tag')
                prop = props.get('prop')
                tval = props.get('valu')
                tcurv = props.get('curv')

                stype_tp = await self._destGetStortype(tagprop=prop)
                if stype_tp is None:
                    err = {'mesg': f'Unable to determine stortype type for tag prop {tag}:{prop}', 'splice': splice}
                    logger.warning(err)
                    return err, None, None

                if spedit == 'tag:prop:set':
                    edit = s_layer.EDIT_TAGPROP_SET
                    edits.append((edit, (tag, prop, tval, tcurv, stype_tp)))

                elif spedit == 'tag:prop:del':
                    edit = s_layer.EDIT_TAGPROP_DEL
                    edits.append((edit, (tag, prop, tval, stype_tp)))

            else:
                err = {'mesg': 'Unrecognized splice edit', 'splice': splice}
                logger.warning(err)
                return err, None, None

        return None, (buid, form, edits), meta

    async def _destGetStortype(self, form=None, prop=None, tagprop=None):
        '''
        Get the stortype integer for a given form, form prop, or tag prop.

        Args:
            form (str or None): Form name
            prop (str or None): Prop name (form must be specified in this case)
            tagprop (str or None): Tag prop name

        Returns:
            (int or None): Stortype integer or None if not found
        '''
        mtype = None

        if form is not None:
            if prop is None:
                mtype = form
            else:
                mtype = self.model['forms'].get(form, {})['props'].get(prop, {})['type'][0]

        elif tagprop is not None:
            mtype = self.model['tagprops'].get(tagprop, {})['type'][0]

        return self.model['types'].get(mtype, {}).get('stortype')

    async def _destPushLyrNodeedits(self, lyriden):
        '''
        Open a proxy to the given destination layer and initiate the queue reader.
        Intended to be run as a fired task.

        Args:
            lyriden (str): Layer iden
        '''
        async with await s_telepath.openurl(os.path.join(self.dest, 'layer', lyriden)) as prx:
            logger.info(f'Starting {lyriden} splice queue reader')
            queue = self._queues.get(lyriden)
            self.push_last_start[lyriden] = s_common.now()
            await self._destIterLyrNodeedits(prx, queue, lyriden)

    async def _destIterLyrNodeedits(self, prx, queue, lyriden):
        '''
        Batch available source splices in a queue as nodeedits and push to the destination layer proxy.
        Nodeedit batch boundaries are defined by the ndef and prov iden.
        Will run as long as queue is not fini'd.

        Args:
            prx (s_telepath.Proxy): Proxy to destination layer
            queue (s_queue.Window): Layer queue for splices
            lyriden (str): Layer iden
        '''
        fair_iter = self.push_fair_iter
        batch_size = self.batch_size
        err_lim = self.err_lim

        ndef = None
        prov = None
        nodesplices = []
        nodeedits = []

        cnt = 0
        errs = 0
        async for offs, splice in queue:
            queuelen = len(queue.linklist)
            next_ndef = splice[1]['ndef']
            next_prov = splice[1].get('prov')

            # current splice is a new node or has new prov iden or the queue is empty
            # so create prior node nodeedit and push to destination layer if at batch size
            if ndef is not None and (next_ndef != ndef or (prov is not None and next_prov != prov) or queuelen == 0):
                err, ne, meta = await self._trnNodeSplicesToNodeedit(ndef, nodesplices)
                if err is None:
                    nodeedits.append((ne, meta))
                else:
                    errs += 1
                    await self._setLyrErr(lyriden, offs, err)
                    if errs >= err_lim:
                        raise Exception('Error limit reached')

                nodesplices = []

                if len(nodeedits) >= batch_size or (queuelen == 0 and len(nodeedits) > 0):
                    # await prx.foo(nodeddits)  # TODO
                    await self._setLyrOffset('push', lyriden, offs + 1)
                    nodeedits = []

            ndef = next_ndef
            prov = next_prov
            nodesplices.append(splice)

            cnt += 1
            if cnt % fair_iter == 0:
                logger.info(f'Yielding {lyriden} queue reader: read={cnt}, errs={errs}, size={len(queue.linklist)}')
                await asyncio.sleep(0)


def getParser():
    https = os.getenv('SYN_UNIV_HTTPS', '4443')
    telep = os.getenv('SYN_UNIV_TELEPATH', 'tcp://0.0.0.0:27492/')
    telen = os.getenv('SYN_UNIV_NAME', None)

    pars = argparse.ArgumentParser(prog='synapse.tools.sync_020')
    s_config.common_argparse(pars, https=https, telep=telep, telen=telen)

    return pars

async def cb(cell, opts, outp):
    await s_config.common_cb(cell, opts, outp)

async def main(argv, outp=s_output.stdout):
    pars = getParser()
    cell = await s_config.main(SyncMigrator, argv, pars=pars, cb=cb, outp=outp)
    return cell

if __name__ == '__main__':  # pragma: no cover
    asyncio.run(s_base.main(main(sys.argv[1:])))
